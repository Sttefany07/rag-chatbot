# rag-chatbot
RAG Chatbot (FastAPI + Chroma + Ollama)

Es un proyecto donde puedes subir documentos (PDF, TXT, MD) y luego hacer preguntas en espaÃ±ol.
El sistema busca la respuesta dentro de esos documentos usando inteligencia artificial (IA).

âš™ï¸ Stack principal

  Python 3.10+ â†’ Lenguaje principal del proyecto.
  FastAPI â†’ Framework para la API (los endpoints como /ask).
  Ollama â†’ Donde corres el LLM (modelo de lenguaje) y los embeddings.
      Modlo de LLM: llama3.1:8b
      Modelo de embeddings: nomic-embed-text
  ChromaDB â†’ Base vectorial que guarda los embeddings de los documentos.
  pypdf â†’ Para leer y extraer texto de archivos PDF.
  langchain_text_splitters â†’ Para partir el texto en chunks (trozos manejables).
  Pydantic â†’ Para definir y validar los schemas de entrada y salida en la API.
  dotenv â†’ Para manejar configuraciones en el archivo .env.

ğŸ› ï¸ Requisitos
  Tener Python 3.10 o mÃ¡s.

Instalar Ollama
  y que estÃ© corriendo en tu PC (ollama serve).

Descargar los modelos en Ollama:
  ollama pull llama3.1:8b
  ollama pull nomic-embed-text:latest


Instalar dependencias:

pip install -r requirements.txt

ğŸ“‚ Carpetas importantes
    app/
     â”œâ”€ embeddings.py   â†’ convierte texto en vectores
     â”œâ”€ retriever.py    â†’ busca en la base vectorial
     â”œâ”€ rag.py          â†’ arma contexto para el LLM
     â”œâ”€ llm.py          â†’ conecta con Ollama
     â”œâ”€ schemas.py      â†’ define cÃ³mo son las requests/responses
     â”œâ”€ ingest.py       â†’ lee PDFs y los guarda en Chroma
     â”œâ”€ settings.py     â†’ configuraciones del proyecto
     â””â”€ main.py         â†’ API con FastAPI
    data/
     â”œâ”€ chroma/         â†’ aquÃ­ se guarda la base de datos vectorial
     â””â”€ uploads/        â†’ aquÃ­ pones tus archivos (PDF, TXT, MD)

âš™ï¸ ConfiguraciÃ³n
Crea un archivo .env en la raÃ­z con esto:

  LLM_PROVIDER=ollama
  OLLAMA_HOST=http://localhost:11434
  OLLAMA_MODEL=llama3.1:8b
  EMBEDDINGS_PROVIDER=ollama
  OLLAMA_EMBED_MODEL=nomic-embed-text
  CHROMA_PATH=./data/chroma
  MAX_CONTEXT_CHUNKS=6
  MMR=true
  CHUNK_SIZE=1200
  CHUNK_OVERLAP=220
  INGEST_BATCH_SIZE=256


ğŸš€ CÃ³mo usarlo
  Levanta la API:
  uvicorn app.main:app --reload --port 8000
  Abre en tu navegador:
  http://localhost:8000/docs

ğŸ§©  CÃ³mo se conecta todo
  ingest.py â†’ lee PDF/TXT, los parte en chunks y los mete en Chroma como embeddings.
  retriever.py â†’ busca en Chroma los chunks mÃ¡s parecidos a la pregunta.
  rag.py â†’ arma el contexto con esos chunks y construye el prompt.
  llm.py â†’ manda el prompt a Ollama y recibe la respuesta del modelo.
  schemas.py â†’ define el formato de requests/responses.
  settings.py â†’ lee la configuraciÃ³n desde .env.
  main.py â†’ une todo y expone la API con FastAPI.
